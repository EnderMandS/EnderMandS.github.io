---
layout: post
title:  "OpenPI"
date:   2025-07-01 00:00:00 +0800
categories: [Paper, VLA]
---

# VLA

## RDT-1B

双臂Diffusion Transformer

对于不同机械臂的异构多模态，将其编码到统一潜在空间中
- 低维输入(感觉, 动作块, 控制频率)通过具有傅里叶特征的MLP编码。感觉, 动作块使用同一个MLP；频率和diffusion time step通过两个MLP编码。将其在MLP长度方向上进行连接，加入位置嵌入区分不同模态并注入时间信息
- 图像SigLIP提取特征，再使用MLP映射到统一空间。为了增强模型根据视角和时间步骤区分图像的能力，将传统的正弦位置嵌入扩展到多维网格，使模型能够捕捉输入图像之间的关系
- 语言T5-XXL，MLP映射到统一空间
- 填充：防止0和速度0混淆，将动作和感觉与一个0-1向量连接起来，表示在将每个维度编码到统一空间之前是否对其进行了填充
所有输入都有10的概率屏蔽

针对不同摄像头的视野不同问题，在编码过程进行随机独立遮蔽，放置过度依赖特定输入

引入KQNorm解决输入的机器人物理量(感觉)的不稳定(传感器异常值, 噪声)导致梯度不稳定、数值溢出问题

引入RMSNorm解决原始DiTs的LayerNorm中的居中操作可能会导致token偏移和注意力偏移，从而破坏时间序列的对称性

MLP解码器：最后的线性层输入MLP解码，，提高灵巧能力

图像和语言作为输入具有高维和长度可变性，使用交叉注意力将其压缩为单个token

图像标记通常比文本标记多得多，同时注入两种模式往往会掩盖与文本相关的信息，策略地在连续的交叉注意力层中注入图像和文本token

统一运动空间，包含感觉和动作，原始动作矢量的每个元素根据其物理意义填充到统一动作空间矢量的相应位置，剩余位置padding

随着历史决策数量的增加，行动预测的误差也会不断累积。这可能会导致机器人偏离训练分布，达到难以恢复的状态。使用动作块(action chunk)减少轨迹中的决策总数，以缓解该问题。

采用6D表示法（Zhou等人，2019年）进行EEF旋转，以解决万向锁

Fine Tunning: 
- 我们为每项任务手动标注指令。为了进一步增强我们的指令并使其与预训练数据集保持一致，我们利用 GPT-4-Turbo为每个任务生成 100 个扩展指令和一个简化指令
- 相同场景不同光照

性能：RTX4090 动作块6Hz，381 action per second

Training:
- 收敛：每隔一段固定时间进行一次diffusion采样，并将采样动作与训练数据集的进行比较。根据经验，我们发现两者的平均平方误差（MSE）与机器人的部署性能之间存在正相关关系。通过观察，可以监控模型的训练进度。当 MSE 收敛时，一般可以停止训练；过低的 MSE 也可能意味着过拟合
- 过拟合：图像增强，包括颜色抖动和图像损坏，并在信噪比（SNR）为 40dB 的输入本体感觉中添加高斯噪声。我们还使用 GPT-4-Turbo 对语言指令进行增强和扩展
- 删除了数据开头的静止部分
- 语言指令是从原始手动注释指令、扩展指令和简化指令中抽取的，抽取概率为三分之一

## pi0

将来自多种类型机器人的数据合并到同一个模型中。流匹配生成高频动作块，使用流匹配损失训练模型

#### 框架 & 实施细节
PaliGemma VLM骨干 + 流匹配的动作专家

输入：语言prompt + 本体感受通过MLP（嵌入flow matching时间步）线性投影到transformer embedding dimension
输出：transformer输出视界范围H的action，并通过线性投影解码

动作专家：
- 拥有两组权重的transformer
- 使用条件流匹配损失监督训练
- 使用完全双向注意力，所有的action token是可见的
- 300M参数

#### 训练数据
加权组合：论文数据集 + OXE
共68项任务，权重为`任务样本数量^0.43`
动作维度18：两个6-DoF机械臂、两个夹爪、底盘、垂直移动躯干
不同任务需要的数据集时长不同，最简单5小时，复杂任务100+小时

#### 实验
五个小时的微调可以实现较好效果

4090一次推理86ms

## pi0.5
任务和感受输入得到子任务，子任务和感受输入得到动作输出

离散动作训练，自回归token采样（FAST）和流匹配迭代生成连续动作

预训练：动作映射到文本token以标准VLM训练方法进行

后训练：添加动作专家，非自回归方式预测连续动作

#### 预训练
数据：
- 多样化移动机械臂：100场景，400小时
- 多样化多环节固定机械臂：单臂、双臂
- 跨实验的实验室数据：与任务相关、不相关；单臂、双臂；移动、静态
- 高级子任务预测：预训练使其能够根据观察和高级任务指令，同时输出子任务和动作。预测子任务之前，预测图像中的边界框
- 多模态网络数据：图像字幕（image caption）、问题解答、对象定位（object localization）。object localization添加室内场景家具边界框标注的网络数据
- 动作：关节姿态、末端执行器姿态。数据集动作维度的1%和99%归一化到[-1,1]

#### 后训练
添加流匹配动作专家，针对家庭情况训练。联合训练

数据：
- 多样化移动机械臂、多样化多环节固定机械臂：筛选长度低于一定阈值的成功序列
- 多模态网络数据：用以保留模型的语义和视觉能力
- 高级子任务预测
- 口头语言指示：由人提供 “语言演示”，选择适当的子任务指挥机器人逐步执行任务。数据通过摇操收集

输入图像随机裁剪、调整大小、旋转和颜色抖动

#### 实机控制
模型直接输出机械臂、夹爪和升降机构的位置、50Hz底盘速度。使用PD控制器

#### 框架
- VLM：PaliGemma 2B
- action tokenization：FAST，预训练时使用
- 动作专家：小transformer，视界50。以流匹配为目标进行训练。噪声动作块首先通过单线性变换层投影到transformer embedding维度。使用一个MLP处理流匹配时间步长，然后使用自适应RMSNorm投影到动作专家的每一层。动作专家的输出tokens通过线性变换投影到目标向量场。后训练和部署使用

##### KQ注意力
- 输入图像、本体状态、Prompt可以相互看见
- FAST单向注意力，只能看到自己以生成的序列
- 动作专家对自己的双向注意力


## PPI

- 关键姿态接口（Keypose Interface）： 在关键帧时刻预测目标抓手姿态，提供明确的空间定位目标。
- 物体点流接口（Object Pointflow Interface）： 基于物体的3D点云数据估计物体在运动过程中的点流信息，用以捕捉物体的细粒度运动变化。 这两个接口协同作用，使得模型既能保持灵活的动作生成，也能确保在执行过程中具备较高的空间感知能力和运动约束遵循能力

#### 框架

输入：自然语言指令、多视角RGBD图像以及机器人的当前状态信息

利用语言提示结合视觉数据（通过 Grounding DINO 来获取目标框，再利用 SAM 模型生成目标遮罩）

DINOv2 模型从各摄像头图像中提取像素级的语义特征，加权融合映射到 3D 空间

为了降低计算负担，同时保留关键信息，采用 PointNet++ 对点云进行编码

中间接口信息：
- 关键抓手姿态：轨迹中那些发生显著变化的关键时刻（关键帧）进行识别，在这些时刻预测出目标抓手的位姿
- 物体点流：目标物体在任务执行过程中的 6D 位姿（BundleSDF 与 Foundation Pose 等方法估计）精细的物体运动信息（点流）。捕捉物体相对于机器人运动中的微小变化，从而增强局部交互的感知能力

输入整合：3D 语义场、语言编码（文本经过 CLIP 编码器处理后得到的特征向量）、机器人状态、初始查询点、两个接口

Diffusion transformer：
- 输入：接口信息和场景、语言与状态信息
- 输出：一段连续行动序列（双臂抓手在整个任务过程中的目标位置、抓握状态及其他运动参数）
- 推理：对初始从高斯分布采样得到的动作进行少次数（仿真中为 1000 步、真实世界中为 20 步等）的反向扩散修正（DDPM），生成最终的动作命令

## BridgeVLA
BridgeVLA 通过「输入–输出对齐」策略，将 3D 点云与预训练的 2D 视觉–语言模型（VLM）无缝衔接, 分为两大阶段：2D Heatmap 预训练、3D Action 细调

### 架构
VLM Backbone：PaliGemma

视觉编码器（SigLIP）：
- 将输入的 2D 图像切分为若干 Patch，映射到固定维度特征向量
- 支持多图像融合，Patch 间采用双向注意力机制与文本前缀融合。

Transformer 主干（Gemma）：
- 输入：所有图像 Patch token + 文本前缀 token
- 更新后的图像 token（用于空间推理）与文本后缀 token（忽略）

2D Heatmap 预训练：  
让 VLM 学会从「图像 + 指令」直接预测空间热力图，而非传统的下一个文本 token
1. 输入：单张 RGB 图像 + 文本提示（例如 “找到所有杯子”）
2. SigLIP 提取图像特征，Gemma 融合文本指令
3. 热图生成头（Convex Upsampling）：将 Transformer 输出的图像 tokens 重新排列成 H×W 特征栅格；通过凸上采样（convex upsampling）模块，生成与输入尺寸一致的 2D 热图
4. 对比由物体检测标注生成的目标热图（基于高斯分布）与预测热图，使用交叉熵训练


3D Action 微调  
将稠密的 3D 输入转为 VLM 可用的 2D 视图、同样预测热图，再将热图投影回 3D 得到动作
1. 数据预处理：
  1. 点云重建：利用 RGB-D 相机融合多视角，重建场景点云
  2. 正交投影从 Top/Front/Right 三个正交方向，各渲染一张深度映射彩色图，每张图视为新的“输入图像”
2. VLM 前向：每个视图与同一条语言指令并行输入 PaliGemma，输出对应的 2D 热图
3. 热图 → 平移动作：
  1. 回投 3D：将三张 2D 热图与相机参数、格点网格对应回 3D 空间，计算每个体素/点的得分
  2. 取极大值：得分最高的 3D 点即为下一个关键帧的位移目标
4. 旋转、夹爪、避碰预测：
  1. 特征抽取：
    - 全局特征：对三张投影图的所有输出 token 做〝全局 max‐pool〞，各得一个向量（共 3）
    - 局部特征：在每张热图的峰值位置截取该处 token（共 3）。
  2. 动作头（MLP）：将 6 个特征向量拼接，过多层感知机，分别输出：
    - Euler 角度（每轴 72 bins 的分类）；
    - 夹爪开/合（二分类）；
    - 避碰标志（二分类）。
5. Coarse-to-Fine
  - 初步预测：在全场景点云上执行一次前向
  - 局部放大：以初步平移点为中心，截取小范围点云，二次前向以提升定位精度
6. 训练损失：总损失 L = L_trans + L_rot + L_grip + L_col
  - L_trans：热图交叉熵
  - L_rot：旋转角分类交叉熵
  - L_grip & L_col：二分类交叉熵

数据增强：随机刚体扰动同时作用于点云与动作标注，提升几何鲁棒性

### 2D 热图生成

#### 1. 构建真实热图  
在预训练阶段，每张输入图像上的“目标对象”由它们的标注边框生成一个或多个高斯概率图，再融合成一张统一的 ground-truth 热图。具体步骤如下：  
1. 对于第 i 个目标，中心点为 xi，定义像素 x=(u,v) 处的初始概率  
   pi(x)=exp(−∥x−xi∥²/(2σ²))。  
2. 对低于阈值 pmin 的像素置零：  
   Hgt_i(x)={ pi(x), if pi(x)≥pmin  
               0,      otherwise  
3. 将所有 N 个目标的概率图按像素位置平均，再归一化到 [0,1]：  
   Havg(x)=1/N ∑₁ⁿ Hgt_i(x)  
   Hgt(x)=Havg(x) / ∑_{x∈Ω} Havg(x)  
这样就得到了与输入同分辨率的 2D ground-truth 热图，用于监督模型训练。

#### 2. 预测热图流程  
1. **视觉–语言融合**  
   将 RGB 图像切分为固定大小的 Patch token，拼接上文本前缀 token，送入 SigLIP + Gemma Transformer。  
2. **重排空间格点**  
   Transformer 输出的 image tokens 按照它们在原图上的位置排成一个 H×W×C 的特征格点。  
3. **交叉熵损失**  
   将最终预测热图与上文构建的 Hgt(x) 做像素级交叉熵，对齐高分区域与目标物体位置。

---

### Convex Upsampling 详解

Convex Upsampling 的目标是在**低分辨率特征格点**与**高分辨率输出热图**之间，建立一组局部、可学习的“凸组合”插值权重，保证上采样后像素值保持平滑且对齐真实结构。

#### 1. 模块结构  
- 输入：L×L 的特征格点，通道数 C。  
- 输出：sL×sL 的热图（s 为上采样倍数，例如 4 或 8）。  
- 核心：一个小型卷积网络生成每个高分辨率像素所需的 k×k 个权重，且这些权重在局部窗口内非负且和为 1（即凸组合）。

#### 2. 上采样流程  
1. **权重预测**  
   在低分辨率格点上滑动一个 k×k 的感受野，通过 1×1 卷积或轻量级 MLP，为对应的 s² 个高分像素位置预测 k² 个凸组合权重。  
2. **特征收集**  
   对应每个高分像素，从其所属的低分辨率格点窗口中收集 k² 个特征向量。  
3. **加权求和**  
   用第 1 步预测的凸组合权重对这些特征加权求和，得到该高分辨率像素的 C 维特征。  
4. **通道整合**  
   将所有高分像素特征拼接成 sL×sL×C 的热图，最后用 1×1 卷积压缩到单通道概率图，并做 Softmax 归一化，输出最终热图。



# Navigation

## Co-NavGPT2
YOLOv8检测目标（云）；Mobile-SAM分割（云）；Fast-LIO2定位（端）；GPT-4o（云）结合俯视图、语义地图、提供全局frontier指引；局部规划策略FFM(Fast Marching Method)


# Control

## HoST
PPO（Proximal Policy Optimization）强化学习。
提供五个历史关节状态
输出action为下一step关节差值，通过PD控制器进行控制
分阶段奖励；奖励分组，每个奖励组为一个独立任务，有自己的评判器；multi-critic强化学习
施加curriculum-based垂直拉力促进探索：在机器人底座上施加一个向上的力，当躯干接近垂直时，力才会生效，这表明机器人已经成功做出了坐地姿势。当机器人能在训练结束时保持目标高度时，力会逐渐减小。
通过rescaler限制动作范围，从而隐式限制关节扭矩和运动速度，比例系数会随着垂直力的减小而逐渐减小
加入平滑正则化L2C2减轻运动震荡
Sim2Real差距：多样化地形；domain randomization域随机化：随机身体质量、基本质心偏移（centor of mass, CoM）、PD 增益、扭矩偏移和初始姿势。CoM 偏移至关重要，因为它能增强控制器对真实世界 CoM 位置噪声的鲁棒性，而这种噪声可能来自扭矩不足或模拟与真实机器人模型之间的差异
缺点：
- 没有外部环境感知，运动时会碰撞
- 仰卧和俯卧姿势的训练对性能产生了负面影响

